{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234afff3",
   "metadata": {},
   "source": [
    "# Geneformer Fine-Tuning for PM2.5 exposure classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34141cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PM2.5 Cell Classification Pipeline using Geneformer\n",
    "Description: Preprocesses single-cell expression data and classifies cells using the Geneformer transformer model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe6178-ea4d-478a-80a8-65ffaa4c1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# scRNA-seq handling\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# HuggingFace Datasets\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "# Geneformer specific\n",
    "from geneformer import TranscriptomeTokenizer, DataCollatorForCellClassification\n",
    "from geneformer.pretrainer import token_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7bb23bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "#!pip install transformers==4.28.0\n",
    "#!pip install --upgrade accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd3b98-5409-4105-b7af-f1ff64ea6a72",
   "metadata": {},
   "source": [
    "# Prepare dataset from raw data to data format for Geneformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5354141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "adata = sc.read_h5ad(\"path/to/your/data.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether adata.X is a sparse matrix\n",
    "if isinstance(adata.X, scipy.sparse.spmatrix):\n",
    "    # Count non-zero elements per cell in a sparse matrix\n",
    "    non_zero_counts = np.diff(adata.X.indptr)\n",
    "else:\n",
    "    # Count non-zero elements per cell in a dense matrix\n",
    "    non_zero_counts = np.count_nonzero(adata.X, axis=1)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create histogram and density plot using seaborn\n",
    "sns.histplot(non_zero_counts, kde=True, bins=50, color='blue')\n",
    "\n",
    "# Set title and axis labels\n",
    "plt.title('Distribution of Non-Zero Gene Counts per Cell')\n",
    "plt.xlabel('Number of Non-Zero Gene Counts')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dccce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ad581",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc15c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete non-homologous genes\n",
    "\n",
    "# Assume you already have an AnnData object named 'adata'\n",
    "# and a list of gene names to be removed named 'genes_to_remove'\n",
    "\n",
    "# Read the gene list to remove\n",
    "genelist_na_df = pd.read_csv(\"./genelist_remove.csv\")\n",
    "\n",
    "# Convert the 'x' column to a list of gene names\n",
    "genes_to_remove = genelist_na_df[\"x\"].tolist()\n",
    "\n",
    "# Identify genes to keep (not in the removal list)\n",
    "genes_to_keep = [gene for gene in adata.var_names if gene not in genes_to_remove]\n",
    "\n",
    "# Subset the AnnData object to retain only the selected genes\n",
    "adata_subset = adata[:, genes_to_keep]\n",
    "\n",
    "# Now 'adata_subset' contains only the genes you want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24acb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "genelist_emsemble_id = pd.read_csv(\"./genelist_ensemle_ids.csv\")\n",
    "genelist_emsemble_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d004e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'genelist' as the index of genelist_emsemble_id (if not already set)\n",
    "genelist_emsemble_id.set_index('genelist', inplace=True)\n",
    "\n",
    "# If the common key in adata_subset.var is not already the index, set 'features' as the index\n",
    "adata_subset.var.set_index('features', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the 'converted_alias' column from genelist_emsemble_id into adata_subset.var\n",
    "adata_subset.var = adata_subset.var.join(genelist_emsemble_id['converted_alias'])\n",
    "\n",
    "# Display the updated var dataframe\n",
    "adata_subset.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset suitable for Geneformer\n",
    "# convert row attributes to Ensembl IDs\n",
    "adata_subset.var.rename(columns={\"converted_alias\": \"ensembl_id\"}, inplace=True)\n",
    "adata_subset.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98994031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any NA values in the 'ensembl_id' column\n",
    "column_name = 'ensembl_id'  \n",
    "\n",
    "na_exists = adata_subset.var[column_name].isnull().any()\n",
    "\n",
    "if na_exists:\n",
    "    print(f\"Column '{column_name}' contains NA values.\")\n",
    "else:\n",
    "    print(f\"Column '{column_name}' does not contain any NA values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ae53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset suitable for Geneformer\n",
    "adata_subset.obs.rename(columns= {\"nCount_RNA\": \"n_counts\"}, inplace=True)\n",
    "adata_subset.obs.rename(columns= {\"sampleid\": \"sample_label\"}, inplace=True)\n",
    "adata_subset.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cell_type and organ_major columns for cell\n",
    "adata_subset.obs['cell_type'] = adata_subset.obs['celltype1']\n",
    "adata_subset.obs['organ_major'] = 'lung'\n",
    "adata_subset.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e02046",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset.write_loom(\"./output_PM25/processed_data.loom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba2283",
   "metadata": {},
   "source": [
    "# Tokenize the dataset into a format compatible with Geneformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7cdbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TranscriptomeTokenizer({\"cell_type\": \"cell_type\", \"organ_major\": \"organ_major\", \"sample_label\": \"sample_label\"}, nproc=8)\n",
    "tk.tokenize_data(\"./output_PM25\", output_directory=\"token_data/\", output_prefix=\"tk_PM25\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c1c74",
   "metadata": {},
   "source": [
    "# Load tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735f1b7-7595-4a02-be17-2c5b970ad81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "dataset=load_from_disk(\"./token_data/tk_PM25_hehai.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad379f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the values in the 'sample_label' column\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Replace values in the 'sample_label' column\n",
    "df['sample_label'] = df['sample_label'].replace({'CONTROL': 'C', 'PM2_5': 'P'})\n",
    "\n",
    "# Convert the modified DataFrame back to a HuggingFace Dataset\n",
    "dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"./token_data/tk_PM25.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ab784",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uniform cell type annotation\n",
    "def unify_Fibroblast(example):\n",
    "    if example[\"cell_type\"] in [\"Fibroblast\", \"Fibroblasts\"]:\n",
    "        example[\"cell_type\"] = \"Fibroblasts\"\n",
    "    return example\n",
    "\n",
    "def unify_Epitheliums(example):\n",
    "    if example[\"cell_type\"] in [\"Epithelium Cells\", \"Epithelial cells\"]:\n",
    "        example[\"cell_type\"] = \"Epitheliums\"\n",
    "    return example\n",
    "\n",
    "def unify_NKs(example):\n",
    "    if example[\"cell_type\"] in [\"NK\", \"NK cells\"]:\n",
    "        example[\"cell_type\"] = \"NKs\"\n",
    "    return example\n",
    "\n",
    "def unify_Endotheliums(example):\n",
    "    if example[\"cell_type\"] in [\"Endothelial Cells\", \"Endothelial cells\"]:\n",
    "        example[\"cell_type\"] = \"Endotheliums\"\n",
    "    return example\n",
    "\n",
    "# Applying transformation functions using the map method\n",
    "dataset = dataset.map(unify_Fibroblast)\n",
    "dataset = dataset.map(unify_Epitheliums)\n",
    "dataset = dataset.map(unify_NKs)\n",
    "dataset = dataset.map(unify_Endotheliums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c50c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cell_types = dataset[\"cell_type\"]\n",
    "\n",
    "cell_type_counts = Counter(cell_types)\n",
    "\n",
    "for cell_type, count in cell_type_counts.items():\n",
    "    print(f\"{cell_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67a21f",
   "metadata": {},
   "source": [
    "# Perform post-processing on the dataset: filtering cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a filtering function for specific cell types\n",
    "cell_types = None  # e.g., [\"Epithelium Cells\"]\n",
    "\n",
    "def filter_cell_types(example):\n",
    "    return example['cell_type'] in cell_types\n",
    "\n",
    "if cell_types is not None:\n",
    "    # Apply the filtering function using the .filter() method\n",
    "    filtered_dataset = dataset.filter(filter_cell_types)\n",
    "else:\n",
    "    # If no filtering is needed, use the original dataset\n",
    "    filtered_dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46c56a",
   "metadata": {},
   "source": [
    "# Perform post-processing on the dataset: padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding to 2048\n",
    "from geneformer.pretrainer import token_dictionary\n",
    "\n",
    "def preprocess_classifier_batch(cell_batch, max_len):\n",
    "    if max_len == None:\n",
    "        max_len = max([len(i) for i in cell_batch[\"input_ids\"]])\n",
    "    def pad_label_example(example):\n",
    "        example[\"input_ids\"] = np.pad(example[\"input_ids\"], \n",
    "                                      (0, max_len-len(example[\"input_ids\"])), \n",
    "                                      mode='constant', constant_values=token_dictionary.get(\"<pad>\"))\n",
    "        example[\"attention_mask\"] = (example[\"input_ids\"] != token_dictionary.get(\"<pad>\")).astype(int)\n",
    "        return example\n",
    "    padded_batch = cell_batch.map(pad_label_example)\n",
    "    return padded_batch\n",
    "\n",
    "# padded to be the same length.\n",
    "set_len=len(dataset)\n",
    "max_set_len = max(dataset.select([i for i in range(set_len)])[\"length\"])\n",
    "padded_dataset = preprocess_classifier_batch(dataset, max_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920524c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset.save_to_disk(\"./token_data/tk_PM25_combined_celltype_revised_padded3.dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570ba6e",
   "metadata": {},
   "source": [
    "# Randomly sample a specified number of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "sampled_indices = random.sample(range(len(padded_dataset)), 2000)\n",
    "sampled_dataset = padded_dataset.select(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ea733",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset = sampled_dataset\n",
    "padded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8599da5",
   "metadata": {},
   "source": [
    "# Map dataset labels: {'C': 0, 'P': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece297c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "evalset_list = []\n",
    "disease_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "disease_list = Counter(padded_dataset[\"cell_type\"]).keys()\n",
    "\n",
    "# shuffle datasets and rename columns\n",
    "trainset_disease_shuffled = padded_dataset.shuffle(seed=42)\n",
    "trainset_disease_shuffled = trainset_disease_shuffled.rename_column(\"sample_label\",\"label\")\n",
    "#trainset_disease_shuffled = trainset_disease_shuffled.remove_columns(\"organ_major\")\n",
    "\n",
    "# create dictionary of cell types : label ids\n",
    "target_name_id_dict = {'C': 0, 'P': 1}\n",
    "print(target_name_id_dict)\n",
    "# change labels to numerical ids\n",
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "labeled_trainset = trainset_disease_shuffled.map(classes_to_ids, num_proc=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb110d-ba43-4efc-bc43-1815d6912647",
   "metadata": {},
   "source": [
    "# Util func:compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy and macro f1 using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece440ca",
   "metadata": {},
   "source": [
    "# Geneformer fine-tuning with 5-fold cross-validation and three data curation strategies; freeze layers 0, 2, and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47423103",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset_list = [padded_dataset1, padded_dataset2, padded_dataset3]\n",
    "freeze_layers_list = [0, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b299e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for freeze_layers in freeze_layers_list:\n",
    "    for i in range(3):\n",
    "        padded_dataset = padded_dataset_list[i]\n",
    "        dataset_number = i+1\n",
    "        \n",
    "        dataset_list = []\n",
    "        evalset_list = []\n",
    "        disease_list = []\n",
    "        target_dict_list = []\n",
    "\n",
    "        disease_list = Counter(padded_dataset[\"cell_type\"]).keys()\n",
    "\n",
    "        # shuffle datasets and rename columns\n",
    "        trainset_disease_shuffled = padded_dataset.shuffle(seed=42)\n",
    "        trainset_disease_shuffled = trainset_disease_shuffled.rename_column(\"sample_label\",\"label\")\n",
    "        #trainset_disease_shuffled = trainset_disease_shuffled.remove_columns(\"organ_major\")\n",
    "\n",
    "        # create dictionary of cell types : label ids\n",
    "        target_name_id_dict = {'C': 0, 'P': 1}\n",
    "        print(target_name_id_dict)\n",
    "        # change labels to numerical ids\n",
    "        def classes_to_ids(example):\n",
    "            example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "            return example\n",
    "        labeled_trainset = trainset_disease_shuffled.map(classes_to_ids, num_proc=16)\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn.model_selection import KFold\n",
    "        import numpy as np\n",
    "\n",
    "        # set model parameters\n",
    "        # max input size\n",
    "        max_input_size = 2 ** 11  # 2048\n",
    "\n",
    "        # set training hyperparameters\n",
    "        # max learning rate\n",
    "        max_lr = 5e-5\n",
    "        # how many pretrained layers to freeze\n",
    "        freeze_layers = freeze_layers\n",
    "        # number gpus\n",
    "        num_gpus = 4\n",
    "        # number cpu cores\n",
    "        num_proc = 32\n",
    "        # batch size for training and eval\n",
    "        geneformer_batch_size = 26\n",
    "        # learning schedule\n",
    "        lr_schedule_fn = \"linear\"\n",
    "        # warmup steps\n",
    "        warmup_steps = 500\n",
    "        # number of epochs\n",
    "        epochs = 50\n",
    "        # optimizer\n",
    "        optimizer = \"adamw\"\n",
    "\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "        splits = list(kf.split(labeled_trainset))\n",
    "\n",
    "\n",
    "        from transformers import BertForSequenceClassification, BertModel, EarlyStoppingCallback\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "        auc_scores = []\n",
    "        roc_curves = []\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(splits):\n",
    "            train_fold = labeled_trainset.select(train_index)\n",
    "            val_fold = labeled_trainset.select(val_index)\n",
    "            \n",
    "            logging_steps = round(len(train_fold)/geneformer_batch_size/30)\n",
    "            print(logging_steps)\n",
    "\n",
    "            # Reload the model\n",
    "            model = BertForSequenceClassification.from_pretrained(\"./Geneformer\", \n",
    "                                                                num_labels=len(target_name_id_dict.keys()),\n",
    "                                                                output_attentions=False,\n",
    "                                                                output_hidden_states=False)\n",
    "            if freeze_layers is not None:\n",
    "                modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n",
    "                for module in modules_to_freeze:\n",
    "                    for param in module.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "            model = model.to(\"cuda\")\n",
    "\n",
    "            # Define output directory path\n",
    "            current_date = datetime.datetime.now()\n",
    "            datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "            output_dir = f\"./models/data_curation/{datestamp}_geneformer_CellClassifier_PM25_dataset{dataset_number}_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_F{freeze_layers}_fold{fold}/\"\n",
    "            \n",
    "            # Ensure not overwriting previously saved model\n",
    "            saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "            if os.path.isfile(saved_model_test):\n",
    "                raise Exception(\"Model already saved to this directory.\")\n",
    "            \n",
    "            # Make output directory\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Set training arguments\n",
    "            training_args = {\n",
    "                \"learning_rate\": max_lr,\n",
    "                \"do_train\": True,\n",
    "                \"do_eval\": True,\n",
    "                \"evaluation_strategy\": \"epoch\",\n",
    "                \"save_strategy\": \"epoch\",\n",
    "                \"logging_steps\": logging_steps,\n",
    "                \"group_by_length\": True,\n",
    "                \"length_column_name\": \"length\",\n",
    "                \"disable_tqdm\": False,\n",
    "                \"lr_scheduler_type\": lr_schedule_fn,\n",
    "                \"warmup_steps\": warmup_steps,\n",
    "                \"weight_decay\": 0.001,\n",
    "                \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "                \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "                \"num_train_epochs\": epochs,\n",
    "                \"load_best_model_at_end\": True,\n",
    "                \"output_dir\": output_dir,\n",
    "            }\n",
    "            \n",
    "            training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "            # Instantiate EarlyStoppingCallback\n",
    "            early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "            \n",
    "            # Create the trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args_init,\n",
    "                data_collator=DataCollatorForCellClassification(),\n",
    "                train_dataset=train_fold,\n",
    "                eval_dataset=val_fold,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[early_stopping]\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate the model\n",
    "            predictions = trainer.predict(val_fold)\n",
    "            \n",
    "            # Compute AUC\n",
    "            y_true = val_fold[\"label\"]\n",
    "            y_scores = predictions.predictions[:, 1]  \n",
    "            auc = roc_auc_score(y_true, y_scores)\n",
    "            auc_scores.append(auc)\n",
    "            \n",
    "            # Compute ROC curve\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "            roc_curves.append((fpr, tpr))\n",
    "            \n",
    "            print(f\"Fold {fold + 1}, AUC: {auc}\")\n",
    "            # Save metrics\n",
    "            with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "                pickle.dump(predictions, fp)\n",
    "            trainer.save_metrics(\"eval\", predictions.metrics)\n",
    "            trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded39ed",
   "metadata": {},
   "source": [
    "# Compare the performance of traditional machine learning models with the fine-tuned Geneformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731d092",
   "metadata": {},
   "source": [
    "## fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa14141",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset = load_from_disk(\"path/to/your/data.dataset\")\n",
    "padded_dataset = padded_dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "dataset_list = []\n",
    "evalset_list = []\n",
    "disease_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "disease_list = Counter(padded_dataset[\"cell_type\"]).keys()\n",
    "\n",
    "# shuffle datasets and rename columns\n",
    "trainset_disease_shuffled = padded_dataset.shuffle(seed=42)\n",
    "trainset_disease_shuffled = trainset_disease_shuffled.rename_column(\"sample_label\",\"label\")\n",
    "#trainset_disease_shuffled = trainset_disease_shuffled.remove_columns(\"organ_major\")\n",
    "\n",
    "# create dictionary of cell types : label ids\n",
    "target_name_id_dict = {'C': 0, 'P': 1}\n",
    "print(target_name_id_dict)\n",
    "# change labels to numerical ids\n",
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "labeled_trainset = trainset_disease_shuffled.map(classes_to_ids, num_proc=16)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# set model parameters\n",
    "max_input_size = 2 ** 11  # 2048\n",
    "max_lr = 5e-5\n",
    "num_gpus = 4\n",
    "num_proc = 32\n",
    "geneformer_batch_size = 26\n",
    "lr_schedule_fn = \"linear\"\n",
    "warmup_steps = 500\n",
    "epochs = 50\n",
    "optimizer = \"adamw\"\n",
    "\n",
    "freeze_layers = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "splits = list(kf.split(labeled_trainset))\n",
    "\n",
    "auc_scores = []\n",
    "roc_curves = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(splits):\n",
    "    train_fold = labeled_trainset.select(train_index)\n",
    "    val_fold = labeled_trainset.select(val_index)\n",
    "    \n",
    "    logging_steps = round(len(train_fold)/geneformer_batch_size/30)\n",
    "    print(logging_steps)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"./Geneformer\", \n",
    "                                                        num_labels=2,\n",
    "                                                        output_attentions = False,\n",
    "                                                        #from_tf=True,\n",
    "                                                        output_hidden_states = False)\n",
    "    if freeze_layers is not None:\n",
    "            modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n",
    "            for module in modules_to_freeze:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    # Define output directory path\n",
    "    current_date = datetime.datetime.now()\n",
    "    datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "    output_dir = f\"./models/5folds_allmodels/1000samples/fine_tuned/fold{fold}/\"\n",
    "    \n",
    "    # Ensure not overwriting previously saved model\n",
    "    saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "    if os.path.isfile(saved_model_test):\n",
    "        raise Exception(\"Model already saved to this directory.\")\n",
    "    \n",
    "    # Make output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set training arguments\n",
    "    training_args = {\n",
    "        \"learning_rate\": max_lr,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"logging_steps\": logging_steps,\n",
    "        \"group_by_length\": True,\n",
    "        \"length_column_name\": \"length\",\n",
    "        \"disable_tqdm\": False,\n",
    "        \"lr_scheduler_type\": lr_schedule_fn,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "        \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "        \"num_train_epochs\": epochs,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"output_dir\": output_dir,\n",
    "    }\n",
    "    \n",
    "    training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "    # Instantiate EarlyStoppingCallback\n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "    \n",
    "    # Create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args_init,\n",
    "        data_collator=DataCollatorForCellClassification(),\n",
    "        train_dataset=train_fold,\n",
    "        eval_dataset=val_fold,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = trainer.predict(val_fold)\n",
    "    \n",
    "    # Compute AUC\n",
    "    y_true = val_fold[\"label\"]\n",
    "    y_scores = predictions.predictions[:, 1]  \n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_curves.append((fpr, tpr))\n",
    "    \n",
    "    print(f\"Fold {fold + 1}, AUC: {auc}\")\n",
    "    # Save metrics\n",
    "    with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(predictions, fp)\n",
    "    trainer.save_metrics(\"eval\", predictions.metrics)\n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669770ba",
   "metadata": {},
   "source": [
    "## 1/2/4/6 layer BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05474535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# Shuffle and select a fixed number of samples for evaluation\n",
    "padded_dataset = padded_dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Initialize containers\n",
    "dataset_list = []\n",
    "evalset_list = []\n",
    "disease_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "# Get list of cell types\n",
    "disease_list = Counter(padded_dataset[\"cell_type\"]).keys()\n",
    "\n",
    "# Shuffle and prepare dataset\n",
    "trainset_disease_shuffled = padded_dataset.shuffle(seed=42)\n",
    "trainset_disease_shuffled = trainset_disease_shuffled.rename_column(\"sample_label\", \"label\")\n",
    "# trainset_disease_shuffled = trainset_disease_shuffled.remove_columns(\"organ_major\")  # optional\n",
    "\n",
    "# Define label mapping\n",
    "target_name_id_dict = {'C': 0, 'P': 1}\n",
    "print(target_name_id_dict)\n",
    "\n",
    "# Convert class labels to numeric ids\n",
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "labeled_trainset = trainset_disease_shuffled.map(classes_to_ids, num_proc=16)\n",
    "\n",
    "# Define model and training hyperparameters\n",
    "max_input_size = 2 ** 11  # 2048\n",
    "max_lr = 5e-5\n",
    "num_gpus = 4\n",
    "num_proc = 32\n",
    "geneformer_batch_size = 26\n",
    "lr_schedule_fn = \"linear\"\n",
    "warmup_steps = 500\n",
    "epochs = 50\n",
    "optimizer = \"adamw\"\n",
    "\n",
    "# Prepare 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "splits = list(kf.split(labeled_trainset))\n",
    "\n",
    "# Store results\n",
    "auc_scores = []\n",
    "roc_curves = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(splits):\n",
    "    train_fold = labeled_trainset.select(train_index)\n",
    "    val_fold = labeled_trainset.select(val_index)\n",
    "\n",
    "    logging_steps = round(len(train_fold) / geneformer_batch_size / 30)\n",
    "    print(f\"Fold {fold + 1} | Logging steps: {logging_steps}\")\n",
    "\n",
    "    num_hidden_layers = 1  # Using a custom lightweight BERT with 1 hidden layer\n",
    "\n",
    "    # Define custom BERT configuration\n",
    "    bert_config = BertConfig(\n",
    "        hidden_size=256,\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        num_attention_heads=4,\n",
    "        intermediate_size=512,\n",
    "        hidden_dropout_prob=0.02,\n",
    "        attention_probs_dropout_prob=0.02,\n",
    "        max_position_embeddings=2048,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True,\n",
    "        classifier_dropout=None,\n",
    "        num_labels=len(target_name_id_dict)\n",
    "    )\n",
    "\n",
    "    # Initialize model and move to GPU\n",
    "    model = BertForSequenceClassification(bert_config)\n",
    "    model.init_weights()\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    # Define output directory path\n",
    "    current_date = datetime.datetime.now()\n",
    "    datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "    output_dir = f\"./models/5folds_allmodels/1000samples/L{num_hidden_layers}/fold{fold}/\"\n",
    "\n",
    "    # Prevent overwriting existing models\n",
    "    saved_model_test = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "    if os.path.isfile(saved_model_test):\n",
    "        raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate=max_lr,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=logging_steps,\n",
    "        group_by_length=True,\n",
    "        length_column_name=\"length\",\n",
    "        disable_tqdm=False,\n",
    "        lr_scheduler_type=lr_schedule_fn,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=0.001,\n",
    "        per_device_train_batch_size=geneformer_batch_size,\n",
    "        per_device_eval_batch_size=geneformer_batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        load_best_model_at_end=True,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Early stopping to avoid overfitting\n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForCellClassification(),\n",
    "        train_dataset=train_fold,\n",
    "        eval_dataset=val_fold,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    predictions = trainer.predict(val_fold)\n",
    "\n",
    "    # Compute AUC score\n",
    "    y_true = val_fold[\"label\"]\n",
    "    y_scores = predictions.predictions[:, 1]  # Assuming binary classification\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_curves.append((fpr, tpr))\n",
    "\n",
    "    print(f\"Fold {fold + 1}, AUC: {auc}\")\n",
    "\n",
    "    # Save predictions and metrics\n",
    "    with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(predictions, fp)\n",
    "    trainer.save_metrics(\"eval\", predictions.metrics)\n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd240c4b",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535421d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "\n",
    "# Machine learning model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "\n",
    "# Persistence\n",
    "import joblib  # For saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9dec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.sample(n=4000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sampled_data.drop(columns=['label'])\n",
    "y = sampled_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24457e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string labels to numeric using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Set up 5-fold stratified cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "# Lists to store TPRs and AUCs for each fold\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=60)\n",
    "\n",
    "# Create directory to save models\n",
    "model_dir = \"./models/5folds_allmodels/max_dataset/random_forest\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "fold = 1\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model on training fold\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(model_dir, f\"random_forest_fold_{fold}.joblib\")\n",
    "    joblib.dump(clf, model_path)\n",
    "    print(f\"Model for fold {fold} saved at {model_path}\")\n",
    "\n",
    "    # Predict class probabilities\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute ROC curve and AUC for this fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))  # Interpolate TPRs to align on mean_fpr\n",
    "    tprs[-1][0] = 0.0  # Ensure the curve starts at (0, 0)\n",
    "    roc_auc = roc_auc_score(y_test, y_probs)\n",
    "    aucs.append(roc_auc)\n",
    "    print(f\"AUC for fold {fold}: {roc_auc:.2f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute mean ROC curve and AUC\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure the curve ends at (1, 1)\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "# Plot mean ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', lw=2,\n",
    "         label=f'Mean ROC (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('roc_curve.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73298f3f",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d61d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold Cross-Validation with Linear SVM\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# Encode string labels into numeric values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Set up 5-fold stratified cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare storage for ROC curves and AUC values\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Create directory to save SVM models\n",
    "model_dir = \"./models/5folds_allmodels/max_dataset/SVM\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "fold = 1\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train linear SVM\n",
    "    linear_svm = LinearSVC(random_state=42)\n",
    "    linear_svm.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(model_dir, f\"linear_svm_fold_{fold}.joblib\")\n",
    "    joblib.dump(linear_svm, model_path)\n",
    "    print(f\"Model for fold {fold} saved at {model_path}\")\n",
    "\n",
    "    # Predict decision scores (used for ROC)\n",
    "    y_probs = linear_svm.decision_function(X_test)\n",
    "\n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = roc_auc_score(y_test, y_probs)\n",
    "    aucs.append(roc_auc)\n",
    "    print(f\"AUC for fold {fold}: {roc_auc:.2f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute average ROC curve and AUC\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "# Plot the averaged ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', lw=2,\n",
    "         label=f'Mean ROC (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - SVM')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('roc_curve_svm.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab3d20",
   "metadata": {},
   "source": [
    "# Cross-cell-type classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a47b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "padded_dataset = load_from_disk(\"path/to/your/data.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd83b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict_list = []\n",
    "# shuffle datasets and rename columns\n",
    "padded_dataset_shuffled = padded_dataset.shuffle(seed=42)\n",
    "padded_dataset_shuffled = padded_dataset_shuffled.rename_column(\"sample_label\",\"label\")\n",
    "#trainset_disease_shuffled = trainset_disease_shuffled.remove_columns(\"organ_major\")\n",
    "\n",
    "# create dictionary of cell types : label ids\n",
    "target_name_id_dict = {'C': 0, 'P': 1}\n",
    "print(target_name_id_dict)\n",
    "# change labels to numerical ids\n",
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "labeled_trainset = padded_dataset_shuffled.map(classes_to_ids, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_list = ['T cells','Macrophages', 'Fibroblasts','Monocytes','Epitheliums','B cells','Neutrophils','NKT','Endotheliums', 'NKs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c977aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertModel, EarlyStoppingCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2 ** 11  # 2048\n",
    "\n",
    "# set training hyperparameters\n",
    "# max learning rate\n",
    "max_lr = 5e-5\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 2\n",
    "# number gpus\n",
    "num_gpus = 4\n",
    "# number cpu cores\n",
    "num_proc = 32\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 26\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = 50\n",
    "# optimizer\n",
    "optimizer = \"adamw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_datasets\n",
    "#miss a function to split datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def split_by_cell_type(dataset):\n",
    "    cell_types = dataset.unique(\"cell_type\")\n",
    "    split_datasets = {}\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        cell_type_dataset = dataset.filter(lambda example: example['cell_type'] == cell_type)\n",
    "        split_datasets[cell_type] = cell_type_dataset\n",
    "    \n",
    "    return split_datasets\n",
    "\n",
    "split_datasets = split_by_cell_type(labeled_trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d69faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_type in cell_type_list:\n",
    "    \n",
    "    test_dataset = split_datasets['NKs']\n",
    "    test_dataset = test_dataset.shuffle(seed=42).select(range(900))\n",
    "    \n",
    "    train_datasets = [split_datasets[ct] for ct in cell_type_list if ct != cell_type]\n",
    "    \n",
    "    train_dataset = concatenate_datasets(train_datasets)\n",
    "    # shuffle datasets\n",
    "    train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "    # create dictionary of cell types : label ids\n",
    "    target_name_id_dict = {'C': 0, 'P': 1}\n",
    "    print(target_name_id_dict)\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "    splits = list(kf.split(labeled_trainset))\n",
    "\n",
    "    auc_scores = []\n",
    "    roc_curves = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(splits):\n",
    "        train_fold = labeled_trainset.select(train_index)\n",
    "        val_fold = labeled_trainset.select(val_index)\n",
    "        \n",
    "        logging_steps = round(len(train_fold)/geneformer_batch_size/30)\n",
    "        print(logging_steps)\n",
    "\n",
    "        # Reload the model\n",
    "        model = BertForSequenceClassification.from_pretrained(\".\", \n",
    "                                                            num_labels=len(target_name_id_dict.keys()),\n",
    "                                                            output_attentions=False,\n",
    "                                                            output_hidden_states=False)\n",
    "        if freeze_layers is not None:\n",
    "            modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n",
    "            for module in modules_to_freeze:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "        # Define output directory path\n",
    "        current_date = datetime.datetime.now()\n",
    "        datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "        output_dir = f\"./models/data_diversity/fine-tuned/{cell_type}/fold{fold}/\"\n",
    "        \n",
    "        # Ensure not overwriting previously saved model\n",
    "        saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "        if os.path.isfile(saved_model_test):\n",
    "            raise Exception(\"Model already saved to this directory.\")\n",
    "        \n",
    "        # Make output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Set training arguments\n",
    "        training_args = {\n",
    "            \"learning_rate\": max_lr,\n",
    "            \"do_train\": True,\n",
    "            \"do_eval\": True,\n",
    "            \"evaluation_strategy\": \"epoch\",\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"logging_steps\": logging_steps,\n",
    "            \"group_by_length\": True,\n",
    "            \"length_column_name\": \"length\",\n",
    "            \"disable_tqdm\": False,\n",
    "            \"lr_scheduler_type\": lr_schedule_fn,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "            \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "            \"num_train_epochs\": epochs,\n",
    "            \"load_best_model_at_end\": True,\n",
    "            \"output_dir\": output_dir,\n",
    "        }\n",
    "        \n",
    "        training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "        # Instantiate EarlyStoppingCallback\n",
    "        early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "        \n",
    "        # Create the trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args_init,\n",
    "            data_collator=DataCollatorForCellClassification(),\n",
    "            train_dataset=train_fold,\n",
    "            eval_dataset=val_fold,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        \n",
    "        # Compute AUC\n",
    "        y_true = test_dataset[\"label\"]\n",
    "        y_scores = predictions.predictions[:, 1]  \n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "        \n",
    "        # Compute ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        roc_curves.append((fpr, tpr))\n",
    "        \n",
    "        print(f\"cell type : {cell_type},Fold {fold + 1}, AUC: {auc}\")\n",
    "        # Save metrics\n",
    "        with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "            pickle.dump(predictions, fp)\n",
    "        trainer.save_metrics(\"eval\", predictions.metrics)\n",
    "        trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6d597",
   "metadata": {},
   "source": [
    "# figures for evaluating cross-cell-type performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2488d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_list = ['T cells','Macrophages', 'Fibroblasts','Monocytes','Epitheliums','B cells',\n",
    " 'Neutrophils','NKT','Endotheliums', 'NKs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define upper directory containing all model runs (e.g., from different data diversity experiments)\n",
    "model_dir_upper = \"./models/data_diversity/L6/\"\n",
    "model_dir_middle = os.listdir(model_dir_upper)\n",
    "\n",
    "# Loop through each subfolder representing a model setting\n",
    "for model_dir in model_dir_middle:\n",
    "    model_dir_1 = model_dir_upper + model_dir + \"/\"\n",
    "\n",
    "    # Paths to 5-fold cross-validation prediction files\n",
    "    prediction_files = [\n",
    "        model_dir_1 + 'fold0/predictions.pickle',\n",
    "        model_dir_1 + 'fold1/predictions.pickle',\n",
    "        model_dir_1 + 'fold2/predictions.pickle',\n",
    "        model_dir_1 + 'fold3/predictions.pickle',\n",
    "        model_dir_1 + 'fold4/predictions.pickle'\n",
    "    ]\n",
    "\n",
    "    all_fpr = []\n",
    "    all_tpr = []\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    # Load predictions and compute individual ROC curves and AUCs\n",
    "    for prediction_file in prediction_files:\n",
    "        with open(prediction_file, 'rb') as f:\n",
    "            predictions = pickle.load(f)\n",
    "\n",
    "        y_true = predictions.label_ids\n",
    "        y_scores = predictions.predictions[:, 1]  # Use probability scores, not class labels\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "        all_fpr.append(fpr)\n",
    "        all_tpr.append(tpr)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # Compute mean ROC curve\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.zeros_like(mean_fpr)\n",
    "    for fpr, tpr in zip(all_fpr, all_tpr):\n",
    "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr /= len(all_fpr)\n",
    "\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(roc_auc_scores)\n",
    "\n",
    "    # Plot each model's average ROC curve\n",
    "    plt.plot(mean_fpr, mean_tpr, label=f'{model_dir} : {mean_auc:.2f} ± {std_auc:.2f}')\n",
    "\n",
    "# Plot diagonal line for random classifier\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "\n",
    "# Plot formatting\n",
    "plt.xlim([0, 1.0])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"./figures/data_diversity_L6.svg\", format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for model_dir in model_dir_middle:\n",
    "    model_dir_1 = model_dir_upper + model_dir + \"/\"\n",
    "    \n",
    "    prediction_files = [\n",
    "        model_dir_1 + 'fold0/predictions.pickle',\n",
    "        model_dir_1 + 'fold1/predictions.pickle',\n",
    "        model_dir_1 + 'fold2/predictions.pickle',\n",
    "        model_dir_1 + 'fold3/predictions.pickle',\n",
    "        model_dir_1 + 'fold4/predictions.pickle']\n",
    "    \n",
    "    all_conf_matrices = []\n",
    "\n",
    "    for prediction_file in prediction_files:\n",
    "        with open(prediction_file, 'rb') as f:\n",
    "            predictions = pickle.load(f)\n",
    "            \n",
    "        y_true = predictions.label_ids\n",
    "        y_pred = np.argmax(predictions.predictions, axis=1) \n",
    "        \n",
    "\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        all_conf_matrices.append(conf_matrix)\n",
    "\n",
    "\n",
    "    mean_conf_matrix = np.mean(all_conf_matrices, axis=0)\n",
    "    \n",
    "\n",
    "    row_sums = mean_conf_matrix.sum(axis=1, keepdims=True)\n",
    "    normalized_conf_matrix = np.round(mean_conf_matrix / row_sums, 2)\n",
    "    \n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=normalized_conf_matrix)\n",
    "    \n",
    "    if model_dir == 'Macrophages':\n",
    "        color = \"Oranges\"\n",
    "    else:\n",
    "        color = \"Blues\"\n",
    "    \n",
    "    disp.plot(cmap=color)\n",
    "\n",
    "\n",
    "    plt.title(f'Average Confusion Matrix for {model_dir}')\n",
    "    \n",
    "\n",
    "    plt.savefig(f\"./figures/{model_dir}_average_confusion_matrix_normalized.svg\", format='svg')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COSMO1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
